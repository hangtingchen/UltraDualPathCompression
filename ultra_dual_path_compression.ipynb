{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Author: Hangting Chen\n",
    "# Copyright: Tencent AI Lab\n",
    "# Paper: Ultra Dual-Path Compression For Joint Echo Cancellation and Noise Suppression\n",
    "# This code give the source of the ultra dual-path compression module and an example of usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define neural network modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WAVSTFT(nn.Module):\n",
    "    def __init__(self, win_size=320):\n",
    "        super(WAVSTFT,self).__init__()\n",
    "        window = torch.from_numpy(np.hanning(win_size).astype(np.float32))\n",
    "        self.window_size = window.shape[-1]\n",
    "        self.hop_length =  self.window_size // 2\n",
    "        window = window.unsqueeze(0).unsqueeze(-1)\n",
    "        divisor = torch.ones(1,1,1,self.window_size*4)\n",
    "        divisor = nn.functional.unfold(divisor,(1,self.window_size),stride=self.hop_length)\n",
    "        divisor = divisor * window.pow(2.0)\n",
    "        divisor = nn.functional.fold(divisor,(1,self.window_size*4),(1,self.window_size),stride=self.hop_length)[:,0,0,:]\n",
    "        divisor = divisor[0,self.window_size:2*self.window_size].unsqueeze(0).unsqueeze(-1)\n",
    "        self.register_buffer('window', window)\n",
    "        self.register_buffer('divisor', divisor)\n",
    "\n",
    "    def magphase(self, complex_tensor: torch.Tensor):\n",
    "        mag = complex_tensor.pow(2.).sum(-1).pow(0.5 * 1.0)\n",
    "        phase = torch.atan2(complex_tensor[..., 1], complex_tensor[..., 0])\n",
    "        return mag, phase\n",
    "\n",
    "    def add_window(self, x, divisor):\n",
    "        out = x * self.window / divisor\n",
    "        return out\n",
    "\n",
    "    def frame(self,x):\n",
    "        assert x.dim()==2, x.shape\n",
    "        out = x.unsqueeze(1).unsqueeze(1)\n",
    "        out = nn.functional.pad(out, (self.window_size, self.window_size), 'constant', 0)\n",
    "        out = nn.functional.unfold(out,(1,self.window_size),\\\n",
    "            stride=self.hop_length) # B N T\n",
    "        return out\n",
    "\n",
    "    def overlap_and_add(self,x,length):\n",
    "        assert x.dim()==3, x.shape\n",
    "        out = nn.functional.fold(x,(1,length+2*self.window_size),(1,self.window_size), \\\n",
    "            stride=self.hop_length)[:,0,0,:]\n",
    "        out = out[:,self.window_size:-self.window_size]\n",
    "        return out\n",
    "\n",
    "    def rfft(self, x):\n",
    "        assert x.dim()==3, x.shape\n",
    "        return torch.fft.rfft(x, dim=1)\n",
    "\n",
    "    def irfft(self, x):\n",
    "        assert x.dim()==3, x.shape\n",
    "        return torch.fft.irfft(x, dim=1)\n",
    "\n",
    "    def STFT(self, x, return_aux=False):\n",
    "        assert x.dim()==2, x.shape\n",
    "        out = self.frame(x)\n",
    "        out = self.add_window(out, 1)\n",
    "        out = self.rfft(out)\n",
    "        if(return_aux):\n",
    "            mag, phase = self.magphase(torch.view_as_real(out))\n",
    "            lps = torch.log(mag**2 + 1e-8)\n",
    "            return out, mag, phase, lps \n",
    "        else:\n",
    "            return out\n",
    "\n",
    "    def iSTFT(self, x, length):\n",
    "        assert x.dim()==3, x.shape\n",
    "        out = self.irfft(x)\n",
    "        out = self.add_window(out, self.divisor)\n",
    "        out = self.overlap_and_add(out, length=length)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeCompression(nn.Module):\n",
    "    def __init__(self, dim1, dim2, dim3, dim4, steps):\n",
    "        super().__init__()\n",
    "        self.dim1 = dim1 # input dim\n",
    "        self.dim2 = dim2 # hidden dim\n",
    "        self.dim3 = dim3 # input dim for decompression\n",
    "        self.dim4 = dim4 # out dim\n",
    "        self.steps = steps\n",
    "        self.trans1 = nn.Conv2d(self.dim1 * self.steps, self.dim2, 1, bias = False)\n",
    "        self.trans2 = nn.Conv2d(self.dim3, self.dim4, 1, bias = False)\n",
    "\n",
    "    def forward(self, x, inverse):\n",
    "        # x B C T F\n",
    "        if(inverse):\n",
    "            B, C, T, F = x.shape\n",
    "            x = self.trans2(x).reshape(B, -1, 1, T, F).permute(0,1,3,2,4).contiguous() # B C T S F\n",
    "            x = x.repeat(1,1,1,self.steps,1).reshape(B, -1, T*self.steps, F) # B C T*S F\n",
    "            x = torch.nn.functional.pad(x,(0,0,self.steps-1,0),'constant',0)\n",
    "            x = x[:,:,:-self.steps+1,:]\n",
    "            if(self.pad > 0): x = x[:,:,self.pad:,:]\n",
    "            return x\n",
    "        else:\n",
    "            B, C, T, F = x.shape\n",
    "            if(x.shape[-2]%self.steps==0):\n",
    "                self.pad = 0\n",
    "            else:\n",
    "                self.pad = self.steps - x.shape[-2]%self.steps\n",
    "                x = torch.nn.functional.pad(x,(0,0,self.pad,0),'constant',0)\n",
    "            x = x.reshape(B, C, -1, self.steps, F).permute(0,1,3,2,4).contiguous() # B C S T F\n",
    "            x = x.reshape(B, C*self.steps, -1, F) # B C*S T F\n",
    "            return self.trans1(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FreqCompression(nn.Module):\n",
    "    def __init__(self, nfreq, nfilters, in_dim, hidden_dim, \\\n",
    "        out_dim, sample_rate=16000):\n",
    "        super().__init__()\n",
    "        self.nfreq = nfreq\n",
    "        self.nfilters = nfilters\n",
    "        self.sample_rate = sample_rate\n",
    "        self.in_dim = in_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.out_dim = out_dim\n",
    "\n",
    "        mel_scale = 'htk'\n",
    "        \n",
    "        all_freqs = torch.linspace(0, sample_rate // 2, nfreq)\n",
    "        # calculate mel freq bins\n",
    "        m_min = self._hz_to_mel(0, mel_scale=mel_scale)\n",
    "        m_max = self._hz_to_mel(sample_rate/2.0, mel_scale=mel_scale)\n",
    "\n",
    "        m_pts = torch.linspace(m_min, m_max, self.nfilters + 2)\n",
    "        f_pts = self._mel_to_hz(m_pts, mel_scale=mel_scale)\n",
    "        self.bounds = [0,]\n",
    "        for freq_inx in range(1, len(f_pts)-1):\n",
    "            self.bounds.append((all_freqs > f_pts[freq_inx]).float().argmax().item())\n",
    "        self.bounds.append(nfreq)\n",
    "        self.trans1 = nn.ModuleList()\n",
    "        self.trans2 = nn.ModuleList()\n",
    "        for freq_inx in range(self.nfilters):\n",
    "            self.trans1.append(nn.Linear((self.bounds[freq_inx+2]-self.bounds[freq_inx])*self.in_dim, self.hidden_dim, bias=False))\n",
    "            self.trans2.append(nn.Conv1d(self.hidden_dim, (self.bounds[freq_inx+2]-self.bounds[freq_inx])*self.out_dim, 1))\n",
    "            \n",
    "    def _hz_to_mel(self, freq: float, mel_scale: str = \"htk\") -> float:\n",
    "        r\"\"\"\n",
    "        Source: https://pytorch.org/audio/stable/_modules/torchaudio/functional/functional.html\n",
    "        Convert Hz to Mels.\n",
    "\n",
    "        Args:\n",
    "            freqs (float): Frequencies in Hz\n",
    "            mel_scale (str, optional): Scale to use: ``htk`` or ``slaney``. (Default: ``htk``)\n",
    "\n",
    "        Returns:\n",
    "            mels (float): Frequency in Mels\n",
    "        \"\"\"\n",
    "\n",
    "        if mel_scale not in [\"slaney\", \"htk\"]:\n",
    "            raise ValueError('mel_scale should be one of \"htk\" or \"slaney\".')\n",
    "\n",
    "        if mel_scale == \"htk\":\n",
    "            return 2595.0 * math.log10(1.0 + (freq / 700.0))\n",
    "\n",
    "        # Fill in the linear part\n",
    "        f_min = 0.0\n",
    "        f_sp = 200.0 / 3\n",
    "\n",
    "        mels = (freq - f_min) / f_sp\n",
    "\n",
    "        # Fill in the log-scale part\n",
    "        min_log_hz = 1000.0\n",
    "        min_log_mel = (min_log_hz - f_min) / f_sp\n",
    "        logstep = math.log(6.4) / 27.0\n",
    "\n",
    "        if freq >= min_log_hz:\n",
    "            mels = min_log_mel + math.log(freq / min_log_hz) / logstep\n",
    "\n",
    "        return mels\n",
    "    \n",
    "    def _mel_to_hz(self, mels: torch.Tensor, mel_scale: str = \"htk\") -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Source: https://pytorch.org/audio/stable/_modules/torchaudio/functional/functional.html\n",
    "        Convert mel bin numbers to frequencies.\n",
    "\n",
    "        Args:\n",
    "            mels (torch.Tensor): Mel frequencies\n",
    "            mel_scale (str, optional): Scale to use: ``htk`` or ``slaney``. (Default: ``htk``)\n",
    "\n",
    "        Returns:\n",
    "            freqs (torch.Tensor): Mels converted in Hz\n",
    "        \"\"\"\n",
    "\n",
    "        if mel_scale not in [\"slaney\", \"htk\"]:\n",
    "            raise ValueError('mel_scale should be one of \"htk\" or \"slaney\".')\n",
    "\n",
    "        if mel_scale == \"htk\":\n",
    "            return 700.0 * (10.0 ** (mels / 2595.0) - 1.0)\n",
    "\n",
    "        # Fill in the linear scale\n",
    "        f_min = 0.0\n",
    "        f_sp = 200.0 / 3\n",
    "        freqs = f_min + f_sp * mels\n",
    "\n",
    "        # And now the nonlinear scale\n",
    "        min_log_hz = 1000.0\n",
    "        min_log_mel = (min_log_hz - f_min) / f_sp\n",
    "        logstep = math.log(6.4) / 27.0\n",
    "\n",
    "        log_t = mels >= min_log_mel\n",
    "        freqs[log_t] = min_log_hz * torch.exp(logstep * (mels[log_t] - min_log_mel))\n",
    "\n",
    "        return freqs\n",
    "    \n",
    "    def forward(self, x, inverse):\n",
    "        if(inverse):\n",
    "            # B C T F\n",
    "            out = torch.zeros([x.shape[0],self.out_dim,self.nfreq,x.shape[2]], dtype=x.dtype, layout=x.layout, device=x.device)\n",
    "            for freq_inx in range(self.nfilters):\n",
    "                out[:,:,self.bounds[freq_inx]:self.bounds[freq_inx+2],:] = out[:,:,self.bounds[freq_inx]:self.bounds[freq_inx+2],:] + \\\n",
    "                    self.trans2[freq_inx](x[:,:,:,freq_inx]).reshape(x.shape[0],self.out_dim,-1,x.shape[-2])\n",
    "            out[:,:,self.bounds[1]:self.bounds[-2],:] = out[:,:,self.bounds[1]:self.bounds[-2],:] / 2.0\n",
    "            out = out.permute(0,1,3,2).contiguous().tanh()\n",
    "            return out\n",
    "        else:\n",
    "            x = x.reshape(x.shape[0],self.in_dim, *x.shape[-2:]) # B C T F\n",
    "            x = x.permute(0,2,1,3).contiguous() # B T C F\n",
    "            x = torch.stack([self.trans1[freq_inx](x[:,:,:,self.bounds[freq_inx]:self.bounds[freq_inx+2]].flatten(start_dim=2)) \\\n",
    "                for freq_inx in range(self.nfilters)],-1) # B T C F\n",
    "            x = x.permute(0,2,1,3).contiguous()\n",
    "            return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UltraDualPath(nn.Module):\n",
    "    def __init__(self, nfreq, in_dim, hidden_dim, out_dim, \\\n",
    "            freq_cprs_ratio, time_cprs_ratio):\n",
    "        super(UltraDualPath, self).__init__()\n",
    "        self.nfreq = nfreq\n",
    "        self.in_dim = in_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.out_dim = out_dim\n",
    "        self.freq_cprs_ratio = freq_cprs_ratio\n",
    "        self.time_cprs_ratio = time_cprs_ratio\n",
    "        self.compress_modules = [\n",
    "            TimeCompression(self.in_dim, self.in_dim * 2, \\\n",
    "                self.out_dim, self.out_dim, time_cprs_ratio), \\\n",
    "            FreqCompression(self.nfreq, self.nfreq // self.freq_cprs_ratio, \\\n",
    "                self.in_dim * 2, self.hidden_dim, self.out_dim), \\\n",
    "        ]\n",
    "    def forward(self, x, inverse):\n",
    "        out = x\n",
    "        # print(out.shape)\n",
    "        if(inverse):\n",
    "            for m in self.compress_modules[::-1]:\n",
    "                out = m(out, inverse)\n",
    "                # print(out.shape)\n",
    "        else:\n",
    "            for m in self.compress_modules:\n",
    "                out = m(out, inverse)\n",
    "                # print(out.shape)\n",
    "        return out"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# An example of run"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define model and input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define network modules\n",
    "win_size = 320\n",
    "wavSTFT = WAVSTFT(win_size) # 16khz, 20ms/10ms -> 320 samples\n",
    "ultraCompress = UltraDualPath(win_size//2 + 1, 3*2, 48, 48, 4, 4) # 3ch*(real+imag)\n",
    "## Define input\n",
    "wav = torch.rand(2,3,16000) # nbatch=2, nchannels=3, nsamples=16000"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run dual path compression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 6, 103, 161])\n"
     ]
    }
   ],
   "source": [
    "# organize input\n",
    "B, C, N = wav.shape\n",
    "spec = wavSTFT.STFT(wav.reshape(B*C, N))\n",
    "spec = torch.view_as_real(spec.reshape(B, C, *spec.shape[-2:])) # B C F T 2\n",
    "spec = spec.permute(0,1,4,3,2) # B C 2 T F\n",
    "spec = spec.reshape(B, C*2, *spec.shape[-2:])\n",
    "print(spec.shape) # nbatch, nchannels*2, nframes, nfreqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 48, 26, 40])\n"
     ]
    }
   ],
   "source": [
    "# compress\n",
    "latent = ultraCompress(spec,0)\n",
    "print(latent.shape) # nbatch, hidden_dim, nframes, nfreqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 48, 103, 161])\n"
     ]
    }
   ],
   "source": [
    "# decompress\n",
    "output = ultraCompress(latent,1)\n",
    "print(output.shape) # nbatch, out_dim, nframes, nfreqs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run freq compression only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 48, 103, 40])\n",
      "torch.Size([2, 48, 103, 161])\n"
     ]
    }
   ],
   "source": [
    "# Frequcy only\n",
    "ultraCompress = FreqCompression(win_size // 2 + 1, (win_size // 2 + 1) // 4, 3 * 2, 48, 48) # 4x compression ratio\n",
    "# compress\n",
    "latent = ultraCompress(spec, 0)\n",
    "print(latent.shape)  # nbatch, hidden_dim, nframes, nfreqs\n",
    "# decompress\n",
    "output = ultraCompress(latent, 1)\n",
    "print(output.shape)  # nbatch, out_dim, nframes, nfreqs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run time compression only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 48, 26, 161])\n",
      "torch.Size([2, 48, 103, 161])\n"
     ]
    }
   ],
   "source": [
    "# Frequcy only\n",
    "ultraCompress = TimeCompression(3 * 2, 48, 48, 48, 4) # 4x compression ratio\n",
    "# compress\n",
    "latent = ultraCompress(spec, 0)\n",
    "print(latent.shape)  # nbatch, hidden_dim, nframes, nfreqs\n",
    "# decompress\n",
    "output = ultraCompress(latent, 1)\n",
    "print(output.shape)  # nbatch, out_dim, nframes, nfreqs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torchcpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
